{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating bias in sentiment analysis using Twitter data\n",
    "### Team member- Divya Damahe, Avani Patel, Andrew Cummings, Bruno De Paula Luiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Using TFIDF for Elastic Net Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this notebook the file \"'fulldatasetT.csv'\" was used as the input file, already cleaned. If you were to download the data from the original site, go to https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset, hydrate the tweets using a Tweet Hydrator, and pass them through the 'tweetProcess' file, which will join the polarity scores with the tweet text. Please refer to tweetProcess for the dates of the tweets used as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "import numpy as np\n",
    "# dataframe functions\n",
    "from pyspark.sql import functions as fn\n",
    "from __future__ import division\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "# we obtain the stop words from a website\n",
    "import requests\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
    "stop_words[0:10]\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "import seaborn\n",
    "from pyspark.ml import feature\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional stop words to add\n",
    "stop_words.append('https')\n",
    "stop_words.append('t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn all scores into positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario 1: turn all scores into positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDFP=pd.read_csv('fulldatasetT.csv',dtype={'polarity': float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDFP=fullDFP.loc[:,['id','text','place','polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDFP['score']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df.loc[data_df[\"mean radius\"] > 12.0, \"mean radius\"] = 0\n",
    "fullDFP.loc[fullDFP[\"polarity\"] > 0.0, 'score'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>place</th>\n",
       "      <th>polarity</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.29E+18</td>\n",
       "      <td>Yoga Instructorüßò‚Äç‚ôÄÔ∏è @bare_table rocks it Frida...</td>\n",
       "      <td>Kentucky, USA</td>\n",
       "      <td>0.1767</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.29E+18</td>\n",
       "      <td>Coz a nice stroll in the city is like a ghost ...</td>\n",
       "      <td>Sydney, New South Wales</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.29E+18</td>\n",
       "      <td>NEW! #Facemasks in my Society6 #Shop üëâ https:/...</td>\n",
       "      <td>Martinique</td>\n",
       "      <td>0.1705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.29E+18</td>\n",
       "      <td>I‚Äôm not saying I‚Äôm bored, but I have invented ...</td>\n",
       "      <td>Boise, ID</td>\n",
       "      <td>-0.1356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.29E+18</td>\n",
       "      <td>Fighting Stigma: NPHET discuss regional lockdo...</td>\n",
       "      <td>Mascouche, Qu√©bec</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151575</th>\n",
       "      <td>1.37E+18</td>\n",
       "      <td>Call us. We‚Äôll keep you safe. Always wear a ma...</td>\n",
       "      <td>Boca Raton, FL</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151576</th>\n",
       "      <td>1.37E+18</td>\n",
       "      <td>What is #themoment you knew the pandemic was c...</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>-0.1519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151577</th>\n",
       "      <td>1.37E+18</td>\n",
       "      <td>I‚Äôve had just about every post-vaccine side ef...</td>\n",
       "      <td>Sapulpa, OK</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151578</th>\n",
       "      <td>1.37E+18</td>\n",
       "      <td>Updated: Closure on #SouthernStateParkway EB a...</td>\n",
       "      <td>North Valley Stream, NY</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151579</th>\n",
       "      <td>1.37E+18</td>\n",
       "      <td>Updated: Closure on #SouthernStateParkway EB a...</td>\n",
       "      <td>North Valley Stream, NY</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151580 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                               text  \\\n",
       "0       1.29E+18  Yoga Instructorüßò‚Äç‚ôÄÔ∏è @bare_table rocks it Frida...   \n",
       "1       1.29E+18  Coz a nice stroll in the city is like a ghost ...   \n",
       "2       1.29E+18  NEW! #Facemasks in my Society6 #Shop üëâ https:/...   \n",
       "3       1.29E+18  I‚Äôm not saying I‚Äôm bored, but I have invented ...   \n",
       "4       1.29E+18  Fighting Stigma: NPHET discuss regional lockdo...   \n",
       "...          ...                                                ...   \n",
       "151575  1.37E+18  Call us. We‚Äôll keep you safe. Always wear a ma...   \n",
       "151576  1.37E+18  What is #themoment you knew the pandemic was c...   \n",
       "151577  1.37E+18  I‚Äôve had just about every post-vaccine side ef...   \n",
       "151578  1.37E+18  Updated: Closure on #SouthernStateParkway EB a...   \n",
       "151579  1.37E+18  Updated: Closure on #SouthernStateParkway EB a...   \n",
       "\n",
       "                          place  polarity  score  \n",
       "0                 Kentucky, USA    0.1767      1  \n",
       "1       Sydney, New South Wales    0.3333      1  \n",
       "2                    Martinique    0.1705      1  \n",
       "3                     Boise, ID   -0.1356      0  \n",
       "4             Mascouche, Qu√©bec    0.0000      0  \n",
       "...                         ...       ...    ...  \n",
       "151575           Boca Raton, FL    0.5000      1  \n",
       "151576            Manhattan, NY   -0.1519      0  \n",
       "151577              Sapulpa, OK    0.0958      1  \n",
       "151578  North Valley Stream, NY    0.0000      0  \n",
       "151579  North Valley Stream, NY    0.0000      0  \n",
       "\n",
       "[151580 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema for dataset from pandas\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "mySchema = StructType([StructField(\"id\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"text\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"place\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"polarity\", FloatType(), True)\\\n",
    "                       \n",
    "                       ,StructField(\"score\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fullDf=spark.createDataFrame(fullDFP,schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------+-----+\n",
      "|      id|                text|               place|polarity|score|\n",
      "+--------+--------------------+--------------------+--------+-----+\n",
      "|1.29E+18|Yoga Instructorüßò...|       Kentucky, USA|  0.1767|    1|\n",
      "|1.29E+18|Coz a nice stroll...|Sydney, New South...|  0.3333|    1|\n",
      "|1.29E+18|NEW! #Facemasks i...|          Martinique|  0.1705|    1|\n",
      "|1.29E+18|Fighting Stigma: ...|   Mascouche, Qu√©bec|  0.1238|    1|\n",
      "|1.29E+18|BON APPETITE\n",
      "\n",
      "Eva...|     Bal Harbour, FL|  0.0111|    1|\n",
      "|1.29E+18|We are probably m...|      Tennessee, USA|  0.0606|    1|\n",
      "|1.29E+18|Great #summer nig...|        Pasadena, CA|     0.6|    1|\n",
      "|1.29E+18|Waited all day to...|         Phoenix, AZ|  0.2964|    1|\n",
      "|1.29E+18|Another new month...|  Navi Mumbai, India|  0.1811|    1|\n",
      "|1.29E+18|@ChelseaFC\n",
      "\n",
      "let's...|       Ogun, Nigeria|     0.5|    1|\n",
      "+--------+--------------------+--------------------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#displaying positive tweets\n",
    "fullDf.where(fn.col('score')==1).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying counts of positive and negative tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|score|count(score)|\n",
      "+-----+------------+\n",
      "|    1|       87090|\n",
      "|    0|       64490|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullDf.groupBy('score').agg(fn.count('score')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sentiment Analysis WITHOUT Elastic Net Regularization LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_reviews_df = spark.read.parquet('imdb_reviews_preprocessed.parquet')\n",
    "sentiments_df=spark.read.parquet('sentiments.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into words \n",
    "tokenizer = RegexTokenizer().setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"words\")\n",
    "review_words_df = tokenizer.transform(fullDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------+-----+--------------------+\n",
      "|      id|                text|               place|polarity|score|               words|\n",
      "+--------+--------------------+--------------------+--------+-----+--------------------+\n",
      "|1.29E+18|Yoga Instructorüßò...|       Kentucky, USA|  0.1767|    1|[yoga, instructor...|\n",
      "|1.29E+18|Coz a nice stroll...|Sydney, New South...|  0.3333|    1|[coz, a, nice, st...|\n",
      "|1.29E+18|NEW! #Facemasks i...|          Martinique|  0.1705|    1|[new, facemasks, ...|\n",
      "|1.29E+18|I‚Äôm not saying I‚Äô...|           Boise, ID| -0.1356|    0|[i, m, not, sayin...|\n",
      "|1.29E+18|Fighting Stigma: ...|   Mascouche, Qu√©bec|     0.0|    0|[fighting, stigma...|\n",
      "|1.29E+18|Fighting Stigma: ...|   Mascouche, Qu√©bec| -0.3125|    0|[fighting, stigma...|\n",
      "|1.29E+18|Fighting Stigma: ...|   Mascouche, Qu√©bec|  0.1238|    1|[fighting, stigma...|\n",
      "|1.29E+18|Them COVID nights...|Vandenberg Villag...|     0.0|    0|[them, covid, nig...|\n",
      "|1.29E+18|Shoot after post ...|Fort Tondiarpet, ...|     0.0|    0|[shoot, after, po...|\n",
      "|1.29E+18|BON APPETITE\n",
      "\n",
      "Eva...|     Bal Harbour, FL|  0.0111|    1|[bon, appetite, e...|\n",
      "|1.29E+18|Been trying to ma...|       Southport, IN| -0.0429|    0|[been, trying, to...|\n",
      "|1.29E+18|Been trying to ma...|       Southport, IN| -0.0429|    0|[been, trying, to...|\n",
      "|1.29E+18|We are probably m...|      Tennessee, USA|  0.0606|    1|[we, are, probabl...|\n",
      "|1.29E+18|Great #summer nig...|        Pasadena, CA|     0.6|    1|[great, summer, n...|\n",
      "|1.29E+18|Waited all day to...|         Phoenix, AZ|  0.2964|    1|[waited, all, day...|\n",
      "|1.29E+18|Another new month...|  Navi Mumbai, India|  0.1811|    1|[another, new, mo...|\n",
      "|1.29E+18|@ChelseaFC\n",
      "\n",
      "let's...|       Ogun, Nigeria|     0.5|    1|[chelseafc, let, ...|\n",
      "|1.29E+18|Before lockdown\n",
      "....|        Sagar, India|     0.0|    0|[before, lockdown...|\n",
      "|1.29E+18|My Mamang pogi......|Manila City, Nati...|  0.0982|    1|[my, mamang, pogi...|\n",
      "|1.29E+18|Evening in Santa ...|      Santa Cruz, CA|     0.0|    0|[evening, in, san...|\n",
      "+--------+--------------------+--------------------+--------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_words_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|      word|      id|sentiment|\n",
      "+----------+--------+---------+\n",
      "|confidence|1.29E+18|        1|\n",
      "|    strong|1.29E+18|        1|\n",
      "|      nice|1.29E+18|        1|\n",
      "|      like|1.29E+18|        1|\n",
      "|protection|1.29E+18|        1|\n",
      "+----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_words_sentiment_df = review_words_df.\\\n",
    "    select('id', fn.explode('words').alias('word')).\\\n",
    "    join(sentiments_df, 'word')\n",
    "tweet_words_sentiment_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+---------+\n",
      "|      id|      avg_sentiment|predicted|\n",
      "+--------+-------------------+---------+\n",
      "|1.31E+18|0.13998100664767332|      1.0|\n",
      "|1.32E+18| 0.1479426096372496|      1.0|\n",
      "|1.30E+18| 0.3137938922566625|      1.0|\n",
      "|1.36E+18|0.12607260726072608|      1.0|\n",
      "|1.37E+18|0.22395476353666896|      1.0|\n",
      "+--------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_sentiment_prediction_df = tweet_words_sentiment_df.\\\n",
    "    groupBy('id').\\\n",
    "    agg(fn.avg('sentiment').alias('avg_sentiment')).\\\n",
    "    withColumn('predicted', fn.when(fn.col('avg_sentiment') > 0, 1.0).otherwise(0.))\n",
    "simple_sentiment_prediction_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.5745594648264578|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display accuracy of the sentiment analysis \n",
    "fullDf.\\\n",
    "    join(simple_sentiment_prediction_df, 'id').\\\n",
    "    select(fn.expr('float(score = predicted)').alias('correct')).\\\n",
    "    select(fn.avg('correct')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with elastic net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, text: string, place: string, polarity: float, score: int, words: array<string>, filtered: array<string>, tf: vector]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop word remover\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "# we will remove words that appear in 5 docs or less\n",
    "cv = CountVectorizer(minTF=1., minDF=5., vocabSize=2**17)\\\n",
    "  .setInputCol(\"filtered\")\\\n",
    "  .setOutputCol(\"tf\")\n",
    "\n",
    "\n",
    "# inital pipeline to look at the size of our vocabulary\n",
    "cv_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv]).fit(fullDf)\n",
    "# now we can make the transformation between the raw text and the counts\n",
    "cv_pipeline.transform(fullDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31661"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of the vocab\n",
    "len(cv_pipeline.stages[-1].vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main idf pipeline with LR\n",
    "idf = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('score').\\\n",
    "    setFeaturesCol('tfidf').\\\n",
    "    setRegParam(0.0).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.)\n",
    "idf_pipeline = Pipeline(stages=[cv_pipeline, idf]).fit(fullDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split set appropriately\n",
    "training_df, validation_df, testing_df = fullDf.randomSplit([0.6, 0.3, 0.1], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline(stages=[idf_pipeline, lr]).fit(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.7764438461368702|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_pipeline.transform(validation_df).\\\n",
    "    select(fn.expr('float(prediction = score)').alias('correct')).\\\n",
    "    select(fn.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = idf_pipeline.stages[0].stages[-1].vocabulary\n",
    "weights = lr_pipeline.stages[-1].coefficients.toArray()\n",
    "coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': weights})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most negatively weighted words, with large weighted values as we have not applied elastic net yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>devastating</td>\n",
       "      <td>-24.531630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27370</th>\n",
       "      <td>hapiness</td>\n",
       "      <td>-22.211299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5072</th>\n",
       "      <td>awful</td>\n",
       "      <td>-21.704040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30500</th>\n",
       "      <td>shutterstock</td>\n",
       "      <td>-20.404987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24886</th>\n",
       "      <td>crooked</td>\n",
       "      <td>-19.937877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30434</th>\n",
       "      <td>burdens</td>\n",
       "      <td>-19.716803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21436</th>\n",
       "      <td>forecasting</td>\n",
       "      <td>-18.989855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17237</th>\n",
       "      <td>turtles</td>\n",
       "      <td>-18.900695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10792</th>\n",
       "      <td>terrifying</td>\n",
       "      <td>-18.582047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14375</th>\n",
       "      <td>fearful</td>\n",
       "      <td>-18.426585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word     weight\n",
       "5201    devastating -24.531630\n",
       "27370      hapiness -22.211299\n",
       "5072          awful -21.704040\n",
       "30500  shutterstock -20.404987\n",
       "24886       crooked -19.937877\n",
       "30434       burdens -19.716803\n",
       "21436   forecasting -18.989855\n",
       "17237       turtles -18.900695\n",
       "10792    terrifying -18.582047\n",
       "14375       fearful -18.426585"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs_df.sort_values('weight').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with elastic net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_par = 0.02\n",
    "alpha_par = 0.3\n",
    "en_lr = LogisticRegression().\\\n",
    "        setLabelCol('score').\\\n",
    "        setFeaturesCol('tfidf').\\\n",
    "        setRegParam(lambda_par).\\\n",
    "        setMaxIter(100).\\\n",
    "        setElasticNetParam(alpha_par)\n",
    "en_lr_estimator = Pipeline(\n",
    "    stages=[tokenizer, sw_filter, cv, idf, en_lr])\n",
    "en_lr_pipeline = en_lr_estimator.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|avg(float((prediction = score)))|\n",
      "+--------------------------------+\n",
      "|              0.8710965947961954|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_lr_pipeline.transform(validation_df).select(fn.avg(fn.expr('float(prediction = score)'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most negatively weighted words after El-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>worst</td>\n",
       "      <td>-0.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>bad</td>\n",
       "      <td>-0.201350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>hate</td>\n",
       "      <td>-0.182759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>crazy</td>\n",
       "      <td>-0.178315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>stupid</td>\n",
       "      <td>-0.170983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>sick</td>\n",
       "      <td>-0.170353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>cold</td>\n",
       "      <td>-0.165276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>soda</td>\n",
       "      <td>-0.162481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6755</th>\n",
       "      <td>goldkilos</td>\n",
       "      <td>-0.151609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>sad</td>\n",
       "      <td>-0.127827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>difficult</td>\n",
       "      <td>-0.126444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>wrong</td>\n",
       "      <td>-0.123174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>savetheworld</td>\n",
       "      <td>-0.117022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>streetphotographer</td>\n",
       "      <td>-0.112605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>chicken</td>\n",
       "      <td>-0.109292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word    weight\n",
       "1317               worst -0.218300\n",
       "443                  bad -0.201350\n",
       "1668                hate -0.182759\n",
       "496                crazy -0.178315\n",
       "2032              stupid -0.170983\n",
       "1136                sick -0.170353\n",
       "986                 cold -0.165276\n",
       "441                 soda -0.162481\n",
       "6755           goldkilos -0.151609\n",
       "997                  sad -0.127827\n",
       "783            difficult -0.126444\n",
       "1404               wrong -0.123174\n",
       "497         savetheworld -0.117022\n",
       "393   streetphotographer -0.112605\n",
       "1019             chicken -0.109292"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_weights = en_lr_pipeline.stages[-1].coefficients.toArray()\n",
    "en_coeffs_df = pd.DataFrame({'word': en_lr_pipeline.stages[2].vocabulary, 'weight': en_weights})\n",
    "en_coeffs_df.sort_values('weight').head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most positively weighted words after El-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>new</td>\n",
       "      <td>0.648823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>happy</td>\n",
       "      <td>0.604511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>good</td>\n",
       "      <td>0.561014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>love</td>\n",
       "      <td>0.544115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>great</td>\n",
       "      <td>0.530618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>best</td>\n",
       "      <td>0.527535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>safe</td>\n",
       "      <td>0.512404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>0.476345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>free</td>\n",
       "      <td>0.417571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>available</td>\n",
       "      <td>0.409601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>latest</td>\n",
       "      <td>0.399358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>live</td>\n",
       "      <td>0.370560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>thanks</td>\n",
       "      <td>0.370140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>amazing</td>\n",
       "      <td>0.367577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>better</td>\n",
       "      <td>0.358128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word    weight\n",
       "5          new  0.648823\n",
       "28       happy  0.604511\n",
       "32        good  0.561014\n",
       "25        love  0.544115\n",
       "39       great  0.530618\n",
       "88        best  0.527535\n",
       "29        safe  0.512404\n",
       "106  beautiful  0.476345\n",
       "92        free  0.417571\n",
       "146  available  0.409601\n",
       "256     latest  0.399358\n",
       "80        live  0.370560\n",
       "96      thanks  0.370140\n",
       "170    amazing  0.367577\n",
       "183     better  0.358128"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_coeffs_df.sort_values('weight', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of words that had 0 weight in our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807051637978901"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_coeffs_df.query('weight == 0.0').shape\n",
    "en_coeffs_df.query('weight == 0.0').shape[0]/en_coeffs_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>covid</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coronavirus</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>home</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>m</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quarantine</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>health</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>stigma</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mask</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>just</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>like</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fighting</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>work</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ireland</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>socialdistancing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>d</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  weight\n",
       "0              covid     0.0\n",
       "4        coronavirus     0.0\n",
       "10              home     0.0\n",
       "11                 m     0.0\n",
       "13        quarantine     0.0\n",
       "14            health     0.0\n",
       "17            stigma     0.0\n",
       "18              mask     0.0\n",
       "20              just     0.0\n",
       "21              like     0.0\n",
       "22          fighting     0.0\n",
       "23              work     0.0\n",
       "24           ireland     0.0\n",
       "26  socialdistancing     0.0\n",
       "30                 d     0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display some of those 0 weight words\n",
    "en_coeffs_df.query('weight == 0.0').head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1\n",
      "Fitting model 2\n",
      "Fitting model 3\n",
      "Fitting model 4\n",
      "Fitting model 5\n",
      "Fitting model 6\n",
      "Fitting model 7\n",
      "Fitting model 8\n",
      "Fitting model 9\n"
     ]
    }
   ],
   "source": [
    "##fit several models varying the elastic net parameters \n",
    "grid = ParamGridBuilder().\\\n",
    "    addGrid(en_lr.regParam, [0., 0.01, 0.02]).\\\n",
    "    addGrid(en_lr.elasticNetParam, [0., 0.2, 0.4]).\\\n",
    "    build()\n",
    "all_models = []\n",
    "for j in range(len(grid)):\n",
    "    print(\"Fitting model {}\".format(j+1))\n",
    "    model = en_lr_estimator.fit(training_df, grid[j])\n",
    "    all_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8185951051574604,\n",
       " 0.8185951051574604,\n",
       " 0.8185951051574604,\n",
       " 0.8496899344558957,\n",
       " 0.899499040010593,\n",
       " 0.8864122878644097,\n",
       " 0.8480568490278728,\n",
       " 0.8866109063624126,\n",
       " 0.8571270937699997]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate the accuracy of each of them:\n",
    "accuracies = [m.\\\n",
    "    transform(validation_df).\\\n",
    "    select(fn.avg(fn.expr('float(score = prediction)')).alias('accuracy')).\\\n",
    "    first().\\\n",
    "    accuracy for m in all_models]\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select model with highest Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_idx = np.argmax(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_8cde316ffe0f', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       " Param(parent='LogisticRegression_8cde316ffe0f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the parameters of our best model\n",
    "grid[best_model_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          accuracy|\n",
      "+------------------+\n",
      "|0.9022833453971179|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = all_models[best_model_idx]\n",
    "# estimate generalization performance\n",
    "best_model.\\\n",
    "    transform(testing_df).\\\n",
    "    select(fn.avg(fn.expr('float(score = prediction)')).alias('accuracy')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 : 0.9503176134309154\n"
     ]
    }
   ],
   "source": [
    "#eval portion\n",
    "\n",
    "evaluator=evaluation.BinaryClassificationEvaluator(labelCol='score')\n",
    "AUC1=evaluator.evaluate(best_model.transform(testing_df))\n",
    "print(\"Model 1 :\", AUC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create a variable that contains our eval metrics\n",
    "predictions = best_model.transform(testing_df)\n",
    "preds_and_labels = predictions.select(['prediction','score']).withColumn('label', fn.col('score').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "[[7750.  522.]\n",
      " [ 963. 5962.]]\n",
      "Precision = 0.9368955512572534\n",
      "Recall = 0.8894754963847125\n",
      "F1 Score for positive tweets 0.9125699146305564\n",
      "F1 Score for negative tweets 0.889253486464315\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary Stats\")\n",
    "print(np.swapaxes(np.flip(metrics.confusionMatrix().toArray()),1,0))\n",
    "print(\"Precision =\",metrics.precision(1.0))\n",
    "print(\"Recall =\",metrics.recall(1.0))\n",
    "print(\"F1 Score for positive tweets\",metrics.fMeasure(1.0) )\n",
    "print(\"F1 Score for negative tweets\",metrics.fMeasure(0.0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEC corpus as input for the Best LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Equity Evaluation Corpus to use as testing data\n",
    "eec =pd.read_csv('Equity-Evaluation-Corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Template</th>\n",
       "      <th>Person</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Emotion word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-En-mystery-05498</td>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-En-mystery-11722</td>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>furious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-En-mystery-11364</td>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>irritated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-En-mystery-14320</td>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>enraged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-En-mystery-14114</td>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>annoyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>2018-En-mystery-12020</td>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8636</th>\n",
       "      <td>2018-En-mystery-14529</td>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>hilarious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8637</th>\n",
       "      <td>2018-En-mystery-16746</td>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>2018-En-mystery-00046</td>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>wonderful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>2018-En-mystery-16664</td>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8640 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID                                     Sentence  \\\n",
       "0     2018-En-mystery-05498                          Alonzo feels angry.   \n",
       "1     2018-En-mystery-11722                        Alonzo feels furious.   \n",
       "2     2018-En-mystery-11364                      Alonzo feels irritated.   \n",
       "3     2018-En-mystery-14320                        Alonzo feels enraged.   \n",
       "4     2018-En-mystery-14114                        Alonzo feels annoyed.   \n",
       "...                     ...                                          ...   \n",
       "8635  2018-En-mystery-12020      The conversation with my mom was funny.   \n",
       "8636  2018-En-mystery-14529  The conversation with my mom was hilarious.   \n",
       "8637  2018-En-mystery-16746    The conversation with my mom was amazing.   \n",
       "8638  2018-En-mystery-00046  The conversation with my mom was wonderful.   \n",
       "8639  2018-En-mystery-16664      The conversation with my mom was great.   \n",
       "\n",
       "                                               Template  Person  Gender  \\\n",
       "0                <person subject> feels <emotion word>.  Alonzo    male   \n",
       "1                <person subject> feels <emotion word>.  Alonzo    male   \n",
       "2                <person subject> feels <emotion word>.  Alonzo    male   \n",
       "3                <person subject> feels <emotion word>.  Alonzo    male   \n",
       "4                <person subject> feels <emotion word>.  Alonzo    male   \n",
       "...                                                 ...     ...     ...   \n",
       "8635  The conversation with <person object> was <emo...  my mom  female   \n",
       "8636  The conversation with <person object> was <emo...  my mom  female   \n",
       "8637  The conversation with <person object> was <emo...  my mom  female   \n",
       "8638  The conversation with <person object> was <emo...  my mom  female   \n",
       "8639  The conversation with <person object> was <emo...  my mom  female   \n",
       "\n",
       "                  Race Emotion Emotion word  \n",
       "0     African-American   anger        angry  \n",
       "1     African-American   anger      furious  \n",
       "2     African-American   anger    irritated  \n",
       "3     African-American   anger      enraged  \n",
       "4     African-American   anger      annoyed  \n",
       "...                ...     ...          ...  \n",
       "8635               NaN     joy        funny  \n",
       "8636               NaN     joy    hilarious  \n",
       "8637               NaN     joy      amazing  \n",
       "8638               NaN     joy    wonderful  \n",
       "8639               NaN     joy        great  \n",
       "\n",
       "[8640 rows x 8 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping other columns\n",
    "eec_test = eec.drop([\"ID\", \"Template\", \"Person\", \"Gender\", \"Race\", \"Emotion\", \"Emotion word\"], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eec_test_spark= spark.createDataFrame(eec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the pandas df to a spark df\n",
    "eec_test_spark= spark.createDataFrame(eec_test)\n",
    "\n",
    "#renaming the input column as \"Sentence\" from \"text\"\n",
    "eec_test_spark = eec_test_spark.withColumnRenamed(\"sentence\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|      id|                text|               place|polarity|score|               words|            filtered|                  tf|               tfidf|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|1.29E+18|!! Khaddi Georget...|       Mumbai, India|     0.5|    1|[khaddi, georgett...|[khaddi, georgett...|(21612,[23,114,25...|(21612,[23,114,25...|[-0.5654271221116...|[0.36229266069669...|       1.0|\n",
      "|1.29E+18|!! sigalert !! a ...|          Corona, CA|  0.1429|    1|[sigalert, a, cra...|[sigalert, crash,...|(21612,[16,47,117...|(21612,[16,47,117...|[-0.8939347319445...|[0.29029850478073...|       1.0|\n",
      "|1.29E+18|!! sigalert !! a ...|          Corona, CA|  0.1429|    1|[sigalert, a, cra...|[sigalert, crash,...|(21612,[16,47,117...|(21612,[16,47,117...|[-0.8939347319445...|[0.29029850478073...|       1.0|\n",
      "|1.29E+18|\" Winner's are ne...|  Navi Mumbai, India|  0.1841|    1|[winner, s, are, ...|[winner, s, quit,...|(21612,[1,5,109,1...|(21612,[1,5,109,1...|[-7.3258396330699...|[6.57873612618045...|       1.0|\n",
      "|1.29E+18|\"...school and de...|       Manhattan, NY|  0.0313|    1|[school, and, dev...|[school, developm...|(21612,[0,10,23,1...|(21612,[0,10,23,1...|[-0.0646645569201...|[0.48383949164797...|       1.0|\n",
      "|1.29E+18|\"...when good men...|    Toronto, Ontario|     0.7|    1|[when, good, men,...|[good, men, emerg...|(21612,[0,2,32,10...|(21612,[0,2,32,10...|[-1.4582736106639...|[0.18873151398343...|       1.0|\n",
      "|1.29E+18|\"A smile is the l...| Kutorejo, Indonesia|     0.4|    1|[a, smile, is, th...|[smile, light, wi...|(21612,[3,79,90,1...|(21612,[3,79,90,1...|[-3.6170516882836...|[0.02615907790485...|       1.0|\n",
      "|1.29E+18|\"A-Frames\" with #...|   Pontnewydd, Wales|     0.0|    0|[a, frames, with,...|[frames, covid, p...|(21612,[0,47,109,...|(21612,[0,47,109,...|[0.98280738435921...|[0.72766490746122...|       0.0|\n",
      "|1.29E+18|\"Abandoned toilet...|Haight Ashbury, S...| -0.0333|    0|[abandoned, toile...|[abandoned, toile...|(21612,[0,30,41,4...|(21612,[0,30,41,4...|[1.12111357759009...|[0.75419521523094...|       0.0|\n",
      "|1.29E+18|\"American science...|         Seattle, WA|     0.0|    0|[american, scienc...|[american, scienc...|(21612,[0,1,12,54...|(21612,[0,1,12,54...|[0.81389246597885...|[0.69293834437085...|       0.0|\n",
      "|1.29E+18|\"Analysts and ind...|Dubai, United Ara...|  0.0417|    1|[analysts, and, i...|[analysts, indust...|(21612,[1,16,47,7...|(21612,[1,16,47,7...|[1.22135358463918...|[0.77230166745459...|       0.0|\n",
      "|1.29E+18|\"Are [children] s...|      Washington, DC|     0.3|    1|[are, children, s...|[children, suscep...|(21612,[1,30,40,4...|(21612,[1,30,40,4...|[-4.1620345167856...|[0.01533695052964...|       1.0|\n",
      "|1.29E+18|\"Askari Bank‚Äôs Pr...|    Lahore, Pakistan|  0.0024|    1|[askari, bank, s,...|[askari, bank, s,...|(21612,[0,1,35,10...|(21612,[0,1,35,10...|[1.47430803352092...|[0.81371130297757...|       0.0|\n",
      "|1.29E+18|\"Back to work. Ba...|         Houston, TX|  0.1321|    1|[back, to, work, ...|[work, ta, work, ...|(21612,[0,1,23,63...|(21612,[0,1,23,63...|[-3.4572601623427...|[0.03055308183948...|       1.0|\n",
      "|1.29E+18|\"Can a Cartoon Ra...|         Seattle, WA|     0.5|    1|[can, a, cartoon,...|[cartoon, raccoon...|(21612,[0,29,970,...|(21612,[0,29,970,...|[-1.1838598243474...|[0.23435889940861...|       1.0|\n",
      "|1.29E+18|\"Covid 19\" came ....|      Trissur, India|     0.0|    0|[covid, came, is,...|[covid, came, ind...|(21612,[0,1,129,3...|(21612,[0,1,129,3...|[1.45590849081750...|[0.81090609133168...|       0.0|\n",
      "|1.29E+18|\"Cum eat it üí¶\" s...|     Los Angeles, CA|  0.5333|    1|[cum, eat, it, sh...|[cum, eat, said, ...|(21612,[0,21,25,5...|(21612,[0,21,25,5...|[-5.5430006084030...|[0.00389949692331...|       1.0|\n",
      "|1.29E+18|\"Data from the CO...|      Washington, DC|  0.3333|    1|[data, from, the,...|[data, covid, sym...|(21612,[0,19,27,5...|(21612,[0,19,27,5...|[-3.0736539300050...|[0.04420718104571...|       1.0|\n",
      "|1.29E+18|\"Don't Be Like Jo...|    Margate, England|     0.0|    0|[don, t, be, like...|[don, like, john,...|(21612,[0,18,21,2...|(21612,[0,18,21,2...|[1.13407701538077...|[0.75659051282267...|       0.0|\n",
      "|1.29E+18|\"Drive Up to Come...|    Indianapolis, IN|  0.3667|    1|[drive, up, to, c...|[drive, come, com...|(21612,[0,55,91,9...|(21612,[0,55,91,9...|[-4.6292888045362...|[0.00966732954807...|       1.0|\n",
      "+--------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_lr_t = best_model.transform(training_df)\n",
    "predictions_lr_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|      id|                text|               place|polarity|score|               words|            filtered|                  tf|               tfidf|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|1.29E+18|\"Covid 19\" came ....|      Trissur, India|     0.0|    0|[covid, came, is,...|[covid, came, ind...|(21612,[0,1,37,43...|(21612,[0,1,37,43...|[1.45590849081750...|[0.81090609133168...|       0.0|\n",
      "|1.29E+18|\"Due to the curre...| Carmona, Calabarzon| -0.0313|    0|[due, to, the, cu...|[current, covid, ...|(21612,[0,69,118,...|(21612,[0,69,118,...|[2.08600881844618...|[0.88953585418001...|       0.0|\n",
      "|1.29E+18|\"Humans of Boraca...|Malay, Western Vi...|     0.0|    0|[humans, of, bora...|[humans, boracay,...|(21612,[3,34,60,6...|(21612,[3,34,60,6...|[0.57118707227422...|[0.63903704073722...|       0.0|\n",
      "|1.29E+18|\"I am so thrilled...|Pretoria, South A...|  0.4167|    1|[i, am, so, thril...|[thrilled, brookl...|(21612,[2,31,132,...|(21612,[2,31,132,...|[-2.2960880630162...|[0.09144746457194...|       1.0|\n",
      "|1.29E+18|\"Just gained 10 h...|   Dhaka, Bangladesh|     0.0|    0|[just, gained, ht...|[just, gained, nh...|(21612,[2,20,41,4...|(21612,[2,20,41,4...|[1.14170412409563...|[0.75799238083254...|       0.0|\n",
      "|1.29E+18|\"Lockdown may hav...|Cape Town, South ...|     0.0|    0|[lockdown, may, h...|[lockdown, kept, ...|(21612,[2,3,132,2...|(21612,[2,3,132,2...|[0.91667468258218...|[0.71436406512577...|       0.0|\n",
      "|1.29E+18|\"Only do what you...|Sholinganallur, I...|     0.4|    1|[only, do, what, ...|[heart, tells, fo...|(21612,[25,114,15...|(21612,[25,114,15...|[-5.6864545568783...|[0.00338013234551...|       1.0|\n",
      "|1.29E+18|\"Tempting fate 20...|Adelaide, South A...| -0.0125|    0|[tempting, fate, ...|[tempting, fate, ...|(21612,[0,1,3,19,...|(21612,[0,1,3,19,...|[-0.5587160244505...|[0.36384459890378...|       1.0|\n",
      "|1.29E+18|\"We'll Be Aight\" ...|       Manhattan, NY| -0.0792|    0|[we, ll, be, aigh...|[ll, aight, taken...|(21612,[0,78,98,1...|(21612,[0,78,98,1...|[1.44353205865982...|[0.80900101513337...|       0.0|\n",
      "|1.29E+18|\"What you do make...|      Nairobi, Kenya|     0.6|    1|[what, you, do, m...|[makes, differenc...|(21612,[0,69,143,...|(21612,[0,69,143,...|[-1.2702496963525...|[0.21921451118631...|       1.0|\n",
      "|1.29E+18|#AD: Free Houston...|         Houston, TX|     0.4|    1|[ad, free, housto...|[ad, free, housto...|(21612,[0,1,14,33...|(21612,[0,1,14,33...|[-1.2762651019576...|[0.21818665650052...|       1.0|\n",
      "|1.29E+18|#AmericanAirlines...|       Charlotte, NC|  0.4667|    1|[americanairlines...|[americanairlines...|(21612,[0,14,88,9...|(21612,[0,14,88,9...|[-4.4231634008331...|[0.01185401986504...|       1.0|\n",
      "|1.29E+18|#BREAKING - Gabe ...|  Corpus Christi, TX| -0.0625|    0|[breaking, gabe, ...|[breaking, gabe, ...|(21612,[0,168,259...|(21612,[0,168,259...|[1.19485285679723...|[0.76760787006373...|       0.0|\n",
      "|1.29E+18|#BREAKINGNEWS\n",
      "#Th...|      Moscow, Russia|  0.2788|    1|[breakingnews, th...|[breakingnews, th...|(21612,[0,4,5,41,...|(21612,[0,4,5,41,...|[-3.9031642452096...|[0.01977886510865...|       1.0|\n",
      "|1.29E+18|#COVID-19 AWARENE...|              Malawi|     0.0|    0|[covid, awareness...|[covid, awareness...|(21612,[0,1416,16...|(21612,[0,1416,16...|[1.19485285679723...|[0.76760787006373...|       0.0|\n",
      "|1.29E+18|#COVID19 #COVID19...|            Honduras|     0.0|    0|[covid, covid, hn...|[covid, covid, hn...|(21612,[0,11,56,2...|(21612,[0,11,56,2...|[1.53396903346679...|[0.82258628917423...|       0.0|\n",
      "|1.29E+18|#COVID19 #Salud #...|            Honduras|     0.0|    0|[covid, salud, ho...|[covid, salud, ho...|(21612,[0,1,84,23...|(21612,[0,1,84,23...|[1.92497280283613...|[0.87269194033083...|       0.0|\n",
      "|1.29E+18|#COVID19 #fork_tr...|Mossel Bay, South...|     0.0|    0|[covid, fork, tra...|[covid, fork, tra...|(21612,[0,47,771,...|(21612,[0,47,771,...|[1.19485285679723...|[0.76760787006373...|       0.0|\n",
      "|1.29E+18|#CaptainAmerica d...|          Boston, MA|    -0.1|    0|[captainamerica, ...|[captainamerica, ...|(21612,[0,11,133,...|(21612,[0,11,133,...|[1.48850759521377...|[0.81585416511845...|       0.0|\n",
      "|1.29E+18|#CaptainAmerica d...|          Boston, MA|    -0.1|    0|[captainamerica, ...|[captainamerica, ...|(21612,[0,133,181...|(21612,[0,133,181...|[1.48850759521377...|[0.81585416511845...|       0.0|\n",
      "+--------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Tranforming the EEC corpus in the Best model\n",
    "predictions_lr = best_model.transform(eec_test_spark)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_raw_predictions_df = predictions_lr.select(\"text\", \"rawPrediction\",\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to pandas dataframe\n",
    "lr_raw_predictions_df = lr_raw_predictions_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "import matplotlib.pyplot as plt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# dataframe functions\n",
    "from pyspark.sql import functions as fn\n",
    "import os\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "import requests\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
    "stop_words[0:10]\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "###import dataset\n",
    "\n",
    "fullDFP=pd.read_csv('fulldatasetT.csv',dtype={'polarity': float})\n",
    "fullDFP=fullDFP.loc[:,['id','text','place','polarity']]\n",
    "fullDFP['score']=0\n",
    "fullDFP.loc[fullDFP[\"polarity\"] > 0.0, 'score'] = 1\n",
    "\n",
    "mySchema = StructType([StructField(\"id\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"text\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"place\", StringType(), True)\\\n",
    "\n",
    "                       ,StructField(\"polarity\", FloatType(), True)\\\n",
    "                       \n",
    "                       ,StructField(\"score\", IntegerType(), True)])\n",
    "fullDf=spark.createDataFrame(fullDFP,schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_df=spark.read.parquet('sentiments.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDf=fullDf.withColumnRenamed('score','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+--------------------+--------+------------------+\n",
      "|summary|         id|                text|               place|polarity|             label|\n",
      "+-------+-----------+--------------------+--------------------+--------+------------------+\n",
      "|  count|     151580|              151580|              151580|  151580|            151580|\n",
      "|   mean|        NaN|                 NaN|                 NaN|     NaN| 0.574548093416018|\n",
      "| stddev|        NaN|                 NaN|                 NaN|     NaN|0.4944129796127013|\n",
      "|    min|   1.27E+18| https://t.co/Xjr...|'s-Hertogenbosch,...|    -1.0|                 0|\n",
      "|    max|Rome, Lazio|ü™Ç[Everything you...|–≠–ª—å–±—Ä—É—Å—Å–∫–∏–π —Ä–∞–π–æ–Ω...|     NaN|                 1|\n",
      "+-------+-----------+--------------------+--------------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullDf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df, testing_df = fullDf.randomSplit([0.6, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|      word|      id|sentiment|\n",
      "+----------+--------+---------+\n",
      "|confidence|1.29E+18|        1|\n",
      "|    strong|1.29E+18|        1|\n",
      "|      nice|1.29E+18|        1|\n",
      "|      like|1.29E+18|        1|\n",
      "|protection|1.29E+18|        1|\n",
      "+----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer().setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"words\")\n",
    "review_words_df = tokenizer.transform(fullDf)\n",
    "tweet_words_sentiment_df = review_words_df.\\\n",
    "    select('id', fn.explode('words').alias('word')).\\\n",
    "    join(sentiments_df, 'word')\n",
    "tweet_words_sentiment_df.show(5)\n",
    "\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "# we will remove words that appear in 5 docs or less\n",
    "cv = CountVectorizer(minTF=1., minDF=5., vocabSize=2**17)\\\n",
    "  .setInputCol(\"filtered\")\\\n",
    "  .setOutputCol(\"tf\")\n",
    "\n",
    "# we now create a pipelined transformer\n",
    "cv_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv]).fit(fullDf)\n",
    "\n",
    "idf = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('label').\\\n",
    "    setFeaturesCol('tfidf').\\\n",
    "    setRegParam(0.0).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.)\n",
    "idf_pipeline = Pipeline(stages=[cv_pipeline, idf]).fit(fullDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier().setLabelCol('label').\\\n",
    "    setFeaturesCol('tfidf')\n",
    "rf_pipeline = Pipeline(stages=[idf_pipeline, rf]).fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6661050064488603"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce = BinaryClassificationEvaluator()\n",
    "bce.evaluate(rf_pipeline.transform(validation_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------------+-----+----------+\n",
      "|                          text|                   probability|label|prediction|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|‚ù§‚ù§ I challenge you to #Save...|[0.6413883477019112,0.35861...|    0|       0.0|\n",
      "|‚ù§‚ù§ I challenge you to #Save...|[0.6413883477019112,0.35861...|    0|       0.0|\n",
      "|‚ù§‚ù§ I challenge you to #Save...|[0.6413883477019112,0.35861...|    0|       0.0|\n",
      "|‚ù§‚ù§ I challenge you to #Save...|[0.6413883477019112,0.35861...|    0|       0.0|\n",
      "|‚ù§‚ù§ I challenge you to #Save...|[0.6413883477019112,0.35861...|    0|       0.0|\n",
      "|‚ù§‚ù§ I challenge you to #Save...|[0.6413883477019112,0.35861...|    0|       0.0|\n",
      "|‚ù§‚ù§ I challenge you to #Save...|[0.6413883477019112,0.35861...|    0|       0.0|\n",
      "|‚ù§‚ù§ I challenge you to #Save...|[0.6413883477019112,0.35861...|    0|       0.0|\n",
      "|‚ù§‚ù§ I challenge you to #Save...|[0.6413883477019112,0.35861...|    0|       0.0|\n",
      "|‚ù§‚ù§ I challenge you to #Save...|[0.6413883477019112,0.35861...|    0|       0.0|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rf_pipeline.transform(testing_df)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",'probability',\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = rf_pipeline.transform(eec_test_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|               words|            filtered|                  tf|               tfidf|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| Alonzo feels angry.|[alonzo, feels, a...|[alonzo, feels, a...|(31663,[564,5497]...|(31663,[564,5497]...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels furi...|[alonzo, feels, f...|[alonzo, feels, f...|(31663,[564,23788...|(31663,[564,23788...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels irri...|[alonzo, feels, i...|[alonzo, feels, i...|(31663,[564,21194...|(31663,[564,21194...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels enra...|[alonzo, feels, e...|[alonzo, feels, e...| (31663,[564],[1.0])|(31663,[564],[5.5...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels anno...|[alonzo, feels, a...|[alonzo, feels, a...|(31663,[564,15616...|(31663,[564,15616...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|   Alonzo feels sad.|[alonzo, feels, sad]|[alonzo, feels, sad]|(31663,[564,959],...|(31663,[564,959],...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels depr...|[alonzo, feels, d...|[alonzo, feels, d...|(31663,[564,7472]...|(31663,[564,7472]...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels deva...|[alonzo, feels, d...|[alonzo, feels, d...|(31663,[564,5805]...|(31663,[564,5805]...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels mise...|[alonzo, feels, m...|[alonzo, feels, m...|(31663,[564,14851...|(31663,[564,14851...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels disa...|[alonzo, feels, d...|[alonzo, feels, d...|(31663,[564,5815]...|(31663,[564,5815]...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels terr...|[alonzo, feels, t...|[alonzo, feels, t...|(31663,[564,13065...|(31663,[564,13065...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels disc...|[alonzo, feels, d...|[alonzo, feels, d...|(31663,[564,21710...|(31663,[564,21710...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels scared.|[alonzo, feels, s...|[alonzo, feels, s...|(31663,[564,3010]...|(31663,[564,3010]...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels anxi...|[alonzo, feels, a...|[alonzo, feels, a...|(31663,[564,3700]...|(31663,[564,3700]...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels fear...|[alonzo, feels, f...|[alonzo, feels, f...|(31663,[564,14469...|(31663,[564,14469...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "| Alonzo feels happy.|[alonzo, feels, h...|[alonzo, feels, h...|(31663,[32,564],[...|(31663,[32,564],[...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels ecst...|[alonzo, feels, e...|[alonzo, feels, e...|(31663,[564,21648...|(31663,[564,21648...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|  Alonzo feels glad.|[alonzo, feels, g...|[alonzo, feels, g...|(31663,[564,732],...|(31663,[564,732],...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels reli...|[alonzo, feels, r...|[alonzo, feels, r...|(31663,[564,8087]...|(31663,[564,8087]...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "|Alonzo feels exci...|[alonzo, feels, e...|[alonzo, feels, e...|(31663,[334,564],...|(31663,[334,564],...|[8.58131452095296...|[0.42906572604764...|       1.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_rf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n",
      "|                      filtered|prediction|                   probability|                 rawPrediction|                          text|                            tf|                         tfidf|                         words|\n",
      "+------------------------------+----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n",
      "|[angles, progress, covid, k...|       0.0|[0.5144604101287003,0.48553...|[10.289208202574008,9.71079...|Angles in progress. #covid1...|(31663,[0,1,2,13,106,124,21...|(31663,[0,1,2,13,106,124,21...|[angles, in, progress, covi...|\n",
      "|[bridgepoint, health, film,...|       0.0|[0.5144604101287003,0.48553...|[10.289208202574008,9.71079...|Bridgepoint Health (film) #...|(31663,[0,1,2,16,86,124,215...|(31663,[0,1,2,16,86,124,215...|[bridgepoint, health, film,...|\n",
      "|[seen, queen, covid, beache...|       0.0|[0.5144604101287003,0.48553...|[10.289208202574008,9.71079...|Seen on Queen. #covid19 - T...|(31663,[0,1,2,86,124,215,34...|(31663,[0,1,2,86,124,215,34...|[seen, on, queen, covid, th...|\n",
      "|[seen, queen, covid, beache...|       0.0|[0.5144604101287003,0.48553...|[10.289208202574008,9.71079...|Seen on Queen. #covid19 - T...|(31663,[0,1,2,86,124,215,34...|(31663,[0,1,2,86,124,215,34...|[seen, on, queen, covid, th...|\n",
      "|[seen, queen, covid, beache...|       0.0|[0.5144604101287003,0.48553...|[10.289208202574008,9.71079...|Seen on Queen. #covid19 - T...|(31663,[0,1,2,86,124,215,34...|(31663,[0,1,2,86,124,215,34...|[seen, on, queen, covid, th...|\n",
      "|[billionshields, challenge,...|       0.0|[0.510713322086298,0.489286...|[10.214266441725961,9.78573...|#BillionShields Challenge i...|(31663,[0,1,3,6,38,42,68,20...|(31663,[0,1,3,6,38,42,68,20...|[billionshields, challenge,...|\n",
      "|[billionshields, challenge,...|       0.0|[0.510713322086298,0.489286...|[10.214266441725961,9.78573...|#BillionShields Challenge i...|(31663,[0,1,6,38,41,68,202,...|(31663,[0,1,6,38,41,68,202,...|[billionshields, challenge,...|\n",
      "|[billionshields, challenge,...|       0.0|[0.510713322086298,0.489286...|[10.214266441725961,9.78573...|#BillionShields Challenge i...|(31663,[0,1,6,38,68,202,449...|(31663,[0,1,6,38,68,202,449...|[billionshields, challenge,...|\n",
      "|[billionshields, challenge,...|       0.0|[0.510713322086298,0.489286...|[10.214266441725961,9.78573...|#BillionShields Challenge i...|(31663,[0,1,6,38,68,202,449...|(31663,[0,1,6,38,68,202,449...|[billionshields, challenge,...|\n",
      "|[challenge, savetheworld, c...|       0.0|[0.6356098591834108,0.36439...|[12.712197183668215,7.28780...|I challenge you to #SaveThe...|(31663,[0,1,6,38,61,202,259...|(31663,[0,1,6,38,61,202,259...|[i, challenge, you, to, sav...|\n",
      "+------------------------------+----------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select (\"filtered\", \"prediction\", \"probability\", \"rawPrediction\", \"text\", \"tf\", \"tfidf\", \"words\")\\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+--------------------+--------+------------------+-------------------+\n",
      "|summary|      id|                text|               place|polarity|             label|         prediction|\n",
      "+-------+--------+--------------------+--------------------+--------+------------------+-------------------+\n",
      "|  count|   15078|               15078|               15078|   15078|             15078|              15078|\n",
      "|   mean|     NaN|                 NaN|                 NaN|     NaN|0.5718928239819605|  0.990449661758854|\n",
      "| stddev|     NaN|                 NaN|                 NaN|     NaN|0.4948208368650665|0.09726128041853342|\n",
      "|    min|1.27E+18|!! Client Diaries...|'s-Hertogenbosch,...|    -1.0|                 0|                0.0|\n",
      "|    max|     NaN|üß∏ Teddy Bears‚Äô P...|–ö–∏—Å–ª–æ–≤–æ–¥—Å–∫, –°—Ç–∞–≤—Ä...|     NaN|                 1|                1.0|\n",
      "+-------+--------+--------------------+--------------------+--------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|      word|      id|sentiment|\n",
      "+----------+--------+---------+\n",
      "|confidence|1.29E+18|        1|\n",
      "|    strong|1.29E+18|        1|\n",
      "|      nice|1.29E+18|        1|\n",
      "|      like|1.29E+18|        1|\n",
      "|protection|1.29E+18|        1|\n",
      "+----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "tokenizer = RegexTokenizer().setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"words\")\n",
    "\n",
    "review_words_df = tokenizer.transform(fullDf)\n",
    "\n",
    "tweet_words_sentiment_df = review_words_df.\\\n",
    "    select('id', fn.explode('words').alias('word')).\\\n",
    "    join(sentiments_df, 'word')\n",
    "tweet_words_sentiment_df.show(5)\n",
    "\n",
    "#stop words\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "#convert label to numeric using binarizer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_numeric\")\n",
    "\n",
    "\n",
    "# we will remove words that appear in 5 docs or less\n",
    "cv = CountVectorizer(minTF=1., minDF=5., vocabSize=2**17).setInputCol(\"filtered\").setOutputCol(\"tf\")\n",
    "\n",
    "# 4. Vectorise features using vectorassembler\n",
    "vecAssembler = VectorAssembler(inputCols=['label_numeric'], outputCol=\"features\")\n",
    "\n",
    "\n",
    "# we now create a pipelined transformer\n",
    "nb_pipeline = Pipeline(stages=[tokenizer,sw_filter,cv,indexer,vecAssembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: id, text, place, polarity, label",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-1b2e09f57822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multinomial\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Make predictions on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: features does not exist. Available: id, text, place, polarity, label"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Initialise the model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "# Fit the model\n",
    "model = nb.fit(training_df)\n",
    "# Make predictions on test data\n",
    "predictions = model.transform(training_df)\n",
    "predictions.select(\"label\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-ca89ee544c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Fit the pipeline to transform the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meec_test_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meec_test_spark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "#Fit the pipeline to transform the data\n",
    "training_df_sp = nb_pipeline.transform(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AUC:  0.7862369298320915\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print (\"Model AUC: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid and Evaluator for Cross Validation\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\n",
    "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "|      id|                text|               place|polarity|label|               words|            filtered|                  tf|label_numeric|            features|\n",
      "+--------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "|1.29E+18| https://t.co/Xjr...|        Evry, France|     0.0|    0|[https, t, co, xj...|[https, t, xjrcwj...|(31663,[0,1,2,28,...|          1.0|(31663,[0,1,2,28,...|\n",
      "|1.29E+18|!! sigalert !! a ...|          Corona, CA|  0.1429|    1|[sigalert, a, cra...|[sigalert, crash,...|(31663,[0,1,18,53...|          0.0|(31663,[0,1,18,53...|\n",
      "|1.29E+18|\" Winner's are ne...|  Navi Mumbai, India|  0.1841|    1|[winner, s, are, ...|[winner, s, quit,...|(31663,[0,1,3,7,1...|          0.0|(31663,[0,1,3,7,1...|\n",
      "|1.29E+18|\"...when good men...|    Toronto, Ontario|     0.7|    1|[when, good, men,...|[good, men, emerg...|(31663,[0,1,2,4,3...|          0.0|(31663,[0,1,2,4,3...|\n",
      "|1.29E+18|\"A smile is the l...| Kutorejo, Indonesia|     0.4|    1|[a, smile, is, th...|[smile, light, wi...|(31663,[0,1,5,81,...|          0.0|(31663,[0,1,5,81,...|\n",
      "|1.29E+18|\"Analysts and ind...|Dubai, United Ara...|  0.0417|    1|[analysts, and, i...|[analysts, indust...|(31663,[0,1,3,18,...|          0.0|(31663,[0,1,3,18,...|\n",
      "|1.29E+18|\"Are [children] s...|      Washington, DC|     0.3|    1|[are, children, s...|[children, suscep...|(31663,[0,1,3,30,...|          0.0|(31663,[0,1,3,30,...|\n",
      "|1.29E+18|\"Back to work. Ba...|         Houston, TX|  0.1321|    1|[back, to, work, ...|[work, ta, work, ...|(31663,[0,1,2,3,2...|          0.0|(31663,[0,1,2,3,2...|\n",
      "|1.29E+18|\"Being with you i...|Egmore Nungambakk...|    0.46|    1|[being, with, you...|[happiness, follo...|(31663,[0,1,27,11...|          0.0|(31663,[0,1,27,11...|\n",
      "|1.29E+18|\"Can a Cartoon Ra...|         Seattle, WA|     0.5|    1|[can, a, cartoon,...|[cartoon, raccoon...|(31663,[0,1,2,31,...|          0.0|(31663,[0,1,2,31,...|\n",
      "|1.29E+18|\"Closed for the f...|            Elko, NV|   0.025|    1|[closed, for, the...|[closed, foreseea...|(31663,[0,1,2,101...|          0.0|(31663,[0,1,2,101...|\n",
      "|1.29E+18|\"Covid 19\" came ....|      Trissur, India|     0.0|    0|[covid, came, is,...|[covid, came, ind...|(31663,[0,1,2,3,3...|          1.0|(31663,[0,1,2,3,3...|\n",
      "|1.29E+18|\"Cum eat it üí¶\" s...|     Los Angeles, CA|  0.5333|    1|[cum, eat, it, sh...|[cum, eat, said, ...|(31663,[0,1,2,23,...|          0.0|(31663,[0,1,2,23,...|\n",
      "|1.29E+18|\"Data from the CO...|      Washington, DC|  0.3333|    1|[data, from, the,...|[data, covid, sym...|(31663,[0,1,2,22,...|          0.0|(31663,[0,1,2,22,...|\n",
      "|1.29E+18|\"Doing what you l...|Salug, Zamboanga ...|     0.7|    1|[doing, what, you...|[doing, like, fre...|(31663,[0,1,23,11...|          0.0|(31663,[0,1,23,11...|\n",
      "|1.29E+18|\"Don't Be Like Jo...|    Margate, England|     0.0|    0|[don, t, be, like...|[don, t, like, jo...|(31663,[0,1,2,20,...|          1.0|(31663,[0,1,2,20,...|\n",
      "|1.29E+18|\"Drive Up to Come...|    Indianapolis, IN|  0.3667|    1|[drive, up, to, c...|[drive, come, com...|(31663,[0,1,2,62,...|          0.0|(31663,[0,1,2,62,...|\n",
      "|1.29E+18|\"Every election i...|       Southgate, MI|     0.5|    1|[every, election,...|[election, determ...|(31663,[0,1,2,22,...|          0.0|(31663,[0,1,2,22,...|\n",
      "|1.29E+18|\"Everyone of us n...|Paranaque City, N...|   0.275|    1|[everyone, of, us...|[needs, care, pro...|(31663,[0,1,5,9,1...|          0.0|(31663,[0,1,5,9,1...|\n",
      "|1.29E+18|\"Face Mask: I pro...|      Gurgaon, India|     0.0|    0|[face, mask, i, p...|[face, mask, prot...|(31663,[0,1,2,20,...|          1.0|(31663,[0,1,2,20,...|\n",
      "+--------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7862369298320915"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Cross-validation\n",
    "cv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "cvModel = cv.fit(training_df)\n",
    "\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(testing_df)\n",
    "\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "evaluator.evaluate(cvPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cross-validation\n",
    "cv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "cvModel = cv.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "\n",
    "cvPredictions = cvModel.transform(testing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate bestModel found from Cross Validation\n",
    "evaluator.evaluate(cvPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "predictions= cvModel.transform(testing_df)\n",
    "\n",
    "#predictions = best_model.transform(testing_df)\n",
    "\n",
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important: need to cast to float type, and order by prediction, else it won't work\n",
    "preds_and_labels = predictions.select(['prediction','label']).withColumn('label', fn.col('label').cast(FloatType())).orderBy('prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only prediction and label columns\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier \n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Stats\")\n",
    "print(np.swapaxes(np.flip(metrics.confusionMatrix().toArray()),1,0))\n",
    "print(\"Precision =\",metrics.precision(1.0))\n",
    "print(\"Recall =\",metrics.recall(1.0))\n",
    "print(\"F1 Score for positive tweets\",metrics.fMeasure(1.0) )\n",
    "print(\"F1 Score for negative tweets\",metrics.fMeasure(0.0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End of Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_raw_predictions_df = predictions_lr.select(\"text\", \"rawPrediction\",\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_raw_predictions_df = lr_raw_predictions_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_raw_predictions_df[\"id\"] = rf_raw_predictions_df.index +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end of eec on naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the Raw prediction values and Creating a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_raw_predictions_df = predictions_rf.select(\"text\", \"rawPrediction\",\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_raw_predictions_df = rf_raw_predictions_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding index column to meger the dataframes in later cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_raw_predictions_df[\"id\"] = rf_raw_predictions_df.index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_raw_predictions_df[\"id\"] = lr_raw_predictions_df.index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=lr_raw_predictions_df.merge(rf_raw_predictions_df, on= \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_x</th>\n",
       "      <th>rawPrediction_x</th>\n",
       "      <th>prediction_x</th>\n",
       "      <th>id</th>\n",
       "      <th>text_y</th>\n",
       "      <th>rawPrediction_y</th>\n",
       "      <th>prediction_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>[1.758761681156161, -1.758761681156161]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>[-0.26419630122863746, 0.26419630122863746]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8636</td>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8636</th>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>[0.47931264253825856, -0.47931264253825856]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8637</td>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8637</th>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>[-1.2568624618888382, 1.2568624618888382]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8638</td>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>[8.171052346622254, 11.828947653377746]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>[-1.1733344977347269, 1.1733344977347269]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8639</td>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>[-1.5774363136664324, 1.5774363136664324]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8640</td>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>[8.154866507877463, 11.845133492122535]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8640 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_x  \\\n",
       "0                             Alonzo feels angry.   \n",
       "1                           Alonzo feels furious.   \n",
       "2                         Alonzo feels irritated.   \n",
       "3                           Alonzo feels enraged.   \n",
       "4                           Alonzo feels annoyed.   \n",
       "...                                           ...   \n",
       "8635      The conversation with my mom was funny.   \n",
       "8636  The conversation with my mom was hilarious.   \n",
       "8637    The conversation with my mom was amazing.   \n",
       "8638  The conversation with my mom was wonderful.   \n",
       "8639      The conversation with my mom was great.   \n",
       "\n",
       "                                  rawPrediction_x  prediction_x    id  \\\n",
       "0         [1.758761681156161, -1.758761681156161]           0.0     1   \n",
       "1       [1.1883767559140836, -1.1883767559140836]           0.0     2   \n",
       "2       [1.1883767559140836, -1.1883767559140836]           0.0     3   \n",
       "3       [1.1883767559140836, -1.1883767559140836]           0.0     4   \n",
       "4       [1.1883767559140836, -1.1883767559140836]           0.0     5   \n",
       "...                                           ...           ...   ...   \n",
       "8635  [-0.26419630122863746, 0.26419630122863746]           1.0  8636   \n",
       "8636  [0.47931264253825856, -0.47931264253825856]           0.0  8637   \n",
       "8637    [-1.2568624618888382, 1.2568624618888382]           1.0  8638   \n",
       "8638    [-1.1733344977347269, 1.1733344977347269]           1.0  8639   \n",
       "8639    [-1.5774363136664324, 1.5774363136664324]           1.0  8640   \n",
       "\n",
       "                                           text_y  \\\n",
       "0                             Alonzo feels angry.   \n",
       "1                           Alonzo feels furious.   \n",
       "2                         Alonzo feels irritated.   \n",
       "3                           Alonzo feels enraged.   \n",
       "4                           Alonzo feels annoyed.   \n",
       "...                                           ...   \n",
       "8635      The conversation with my mom was funny.   \n",
       "8636  The conversation with my mom was hilarious.   \n",
       "8637    The conversation with my mom was amazing.   \n",
       "8638  The conversation with my mom was wonderful.   \n",
       "8639      The conversation with my mom was great.   \n",
       "\n",
       "                              rawPrediction_y  prediction_y  \n",
       "0     [8.581314520952962, 11.418685479047037]           1.0  \n",
       "1     [8.581314520952962, 11.418685479047037]           1.0  \n",
       "2     [8.581314520952962, 11.418685479047037]           1.0  \n",
       "3     [8.581314520952962, 11.418685479047037]           1.0  \n",
       "4     [8.581314520952962, 11.418685479047037]           1.0  \n",
       "...                                       ...           ...  \n",
       "8635  [8.581314520952962, 11.418685479047037]           1.0  \n",
       "8636  [8.581314520952962, 11.418685479047037]           1.0  \n",
       "8637  [8.171052346622254, 11.828947653377746]           1.0  \n",
       "8638  [8.581314520952962, 11.418685479047037]           1.0  \n",
       "8639  [8.154866507877463, 11.845133492122535]           1.0  \n",
       "\n",
       "[8640 rows x 7 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x = new_df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "new_df = pd.new_df(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"new_val1\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_df)):\n",
    "    new_df.iloc[i,7]= new_df[\"rawPrediction_x\"][i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"new_val2\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_df)):\n",
    "    new_df.iloc[i,8]= new_df[\"rawPrediction_y\"][i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_x</th>\n",
       "      <th>rawPrediction_x</th>\n",
       "      <th>prediction_x</th>\n",
       "      <th>id</th>\n",
       "      <th>text_y</th>\n",
       "      <th>rawPrediction_y</th>\n",
       "      <th>prediction_y</th>\n",
       "      <th>new_val1</th>\n",
       "      <th>new_val2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>[1.758761681156161, -1.758761681156161]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.758762</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>[-0.26419630122863746, 0.26419630122863746]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8636</td>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.264196</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8636</th>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>[0.47931264253825856, -0.47931264253825856]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8637</td>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479313</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8637</th>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>[-1.2568624618888382, 1.2568624618888382]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8638</td>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>[8.171052346622254, 11.828947653377746]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.256862</td>\n",
       "      <td>8.171052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>[-1.1733344977347269, 1.1733344977347269]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8639</td>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.173334</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>[-1.5774363136664324, 1.5774363136664324]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8640</td>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>[8.154866507877463, 11.845133492122535]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.577436</td>\n",
       "      <td>8.154867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8640 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_x  \\\n",
       "0                             Alonzo feels angry.   \n",
       "1                           Alonzo feels furious.   \n",
       "2                         Alonzo feels irritated.   \n",
       "3                           Alonzo feels enraged.   \n",
       "4                           Alonzo feels annoyed.   \n",
       "...                                           ...   \n",
       "8635      The conversation with my mom was funny.   \n",
       "8636  The conversation with my mom was hilarious.   \n",
       "8637    The conversation with my mom was amazing.   \n",
       "8638  The conversation with my mom was wonderful.   \n",
       "8639      The conversation with my mom was great.   \n",
       "\n",
       "                                  rawPrediction_x  prediction_x    id  \\\n",
       "0         [1.758761681156161, -1.758761681156161]           0.0     1   \n",
       "1       [1.1883767559140836, -1.1883767559140836]           0.0     2   \n",
       "2       [1.1883767559140836, -1.1883767559140836]           0.0     3   \n",
       "3       [1.1883767559140836, -1.1883767559140836]           0.0     4   \n",
       "4       [1.1883767559140836, -1.1883767559140836]           0.0     5   \n",
       "...                                           ...           ...   ...   \n",
       "8635  [-0.26419630122863746, 0.26419630122863746]           1.0  8636   \n",
       "8636  [0.47931264253825856, -0.47931264253825856]           0.0  8637   \n",
       "8637    [-1.2568624618888382, 1.2568624618888382]           1.0  8638   \n",
       "8638    [-1.1733344977347269, 1.1733344977347269]           1.0  8639   \n",
       "8639    [-1.5774363136664324, 1.5774363136664324]           1.0  8640   \n",
       "\n",
       "                                           text_y  \\\n",
       "0                             Alonzo feels angry.   \n",
       "1                           Alonzo feels furious.   \n",
       "2                         Alonzo feels irritated.   \n",
       "3                           Alonzo feels enraged.   \n",
       "4                           Alonzo feels annoyed.   \n",
       "...                                           ...   \n",
       "8635      The conversation with my mom was funny.   \n",
       "8636  The conversation with my mom was hilarious.   \n",
       "8637    The conversation with my mom was amazing.   \n",
       "8638  The conversation with my mom was wonderful.   \n",
       "8639      The conversation with my mom was great.   \n",
       "\n",
       "                              rawPrediction_y  prediction_y  new_val1  \\\n",
       "0     [8.581314520952962, 11.418685479047037]           1.0  1.758762   \n",
       "1     [8.581314520952962, 11.418685479047037]           1.0  1.188377   \n",
       "2     [8.581314520952962, 11.418685479047037]           1.0  1.188377   \n",
       "3     [8.581314520952962, 11.418685479047037]           1.0  1.188377   \n",
       "4     [8.581314520952962, 11.418685479047037]           1.0  1.188377   \n",
       "...                                       ...           ...       ...   \n",
       "8635  [8.581314520952962, 11.418685479047037]           1.0 -0.264196   \n",
       "8636  [8.581314520952962, 11.418685479047037]           1.0  0.479313   \n",
       "8637  [8.171052346622254, 11.828947653377746]           1.0 -1.256862   \n",
       "8638  [8.581314520952962, 11.418685479047037]           1.0 -1.173334   \n",
       "8639  [8.154866507877463, 11.845133492122535]           1.0 -1.577436   \n",
       "\n",
       "      new_val2  \n",
       "0     8.581315  \n",
       "1     8.581315  \n",
       "2     8.581315  \n",
       "3     8.581315  \n",
       "4     8.581315  \n",
       "...        ...  \n",
       "8635  8.581315  \n",
       "8636  8.581315  \n",
       "8637  8.171052  \n",
       "8638  8.581315  \n",
       "8639  8.154867  \n",
       "\n",
       "[8640 rows x 9 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_to_be = new_df.loc[:, [\"new_val1\",\"new_val2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_val1</th>\n",
       "      <th>new_val2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.758762</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>-0.264196</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8636</th>\n",
       "      <td>0.479313</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8637</th>\n",
       "      <td>-1.256862</td>\n",
       "      <td>8.171052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>-1.173334</td>\n",
       "      <td>8.581315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>-1.577436</td>\n",
       "      <td>8.154867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8640 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      new_val1  new_val2\n",
       "0     1.758762  8.581315\n",
       "1     1.188377  8.581315\n",
       "2     1.188377  8.581315\n",
       "3     1.188377  8.581315\n",
       "4     1.188377  8.581315\n",
       "...        ...       ...\n",
       "8635 -0.264196  8.581315\n",
       "8636  0.479313  8.581315\n",
       "8637 -1.256862  8.171052\n",
       "8638 -1.173334  8.581315\n",
       "8639 -1.577436  8.154867\n",
       "\n",
       "[8640 rows x 2 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_to_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(scaler.fit_transform(new_df_to_be),columns=new_df_to_be.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_val1</th>\n",
       "      <th>new_val2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.524358</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>0.236106</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8636</th>\n",
       "      <td>0.342049</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8637</th>\n",
       "      <td>0.094661</td>\n",
       "      <td>0.037955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>0.106563</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>0.048983</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8640 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      new_val1  new_val2\n",
       "0     0.524358  1.000000\n",
       "1     0.443084  1.000000\n",
       "2     0.443084  1.000000\n",
       "3     0.443084  1.000000\n",
       "4     0.443084  1.000000\n",
       "...        ...       ...\n",
       "8635  0.236106  1.000000\n",
       "8636  0.342049  1.000000\n",
       "8637  0.094661  0.037955\n",
       "8638  0.106563  1.000000\n",
       "8639  0.048983  0.000000\n",
       "\n",
       "[8640 rows x 2 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled[\"id\"] = df_scaled.index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_val1</th>\n",
       "      <th>new_val2</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.524358</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>0.236106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8636</th>\n",
       "      <td>0.342049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8637</th>\n",
       "      <td>0.094661</td>\n",
       "      <td>0.037955</td>\n",
       "      <td>8638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>0.106563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>0.048983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8640 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      new_val1  new_val2    id\n",
       "0     0.524358  1.000000     1\n",
       "1     0.443084  1.000000     2\n",
       "2     0.443084  1.000000     3\n",
       "3     0.443084  1.000000     4\n",
       "4     0.443084  1.000000     5\n",
       "...        ...       ...   ...\n",
       "8635  0.236106  1.000000  8636\n",
       "8636  0.342049  1.000000  8637\n",
       "8637  0.094661  0.037955  8638\n",
       "8638  0.106563  1.000000  8639\n",
       "8639  0.048983  0.000000  8640\n",
       "\n",
       "[8640 rows x 3 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=new_df.merge(df_scaled, on= \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_x</th>\n",
       "      <th>rawPrediction_x</th>\n",
       "      <th>prediction_x</th>\n",
       "      <th>id</th>\n",
       "      <th>text_y</th>\n",
       "      <th>rawPrediction_y</th>\n",
       "      <th>prediction_y</th>\n",
       "      <th>new_val1_x</th>\n",
       "      <th>new_val2_x</th>\n",
       "      <th>new_val1_y</th>\n",
       "      <th>new_val2_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>[1.758761681156161, -1.758761681156161]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.758762</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.524358</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.443084</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>[-0.26419630122863746, 0.26419630122863746]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8636</td>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.264196</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.236106</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8636</th>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>[0.47931264253825856, -0.47931264253825856]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8637</td>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479313</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.342049</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8637</th>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>[-1.2568624618888382, 1.2568624618888382]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8638</td>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>[8.171052346622254, 11.828947653377746]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.256862</td>\n",
       "      <td>8.171052</td>\n",
       "      <td>0.094661</td>\n",
       "      <td>0.037955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>[-1.1733344977347269, 1.1733344977347269]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8639</td>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.173334</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.106563</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>[-1.5774363136664324, 1.5774363136664324]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8640</td>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>[8.154866507877463, 11.845133492122535]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.577436</td>\n",
       "      <td>8.154867</td>\n",
       "      <td>0.048983</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8640 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_x  \\\n",
       "0                             Alonzo feels angry.   \n",
       "1                           Alonzo feels furious.   \n",
       "2                         Alonzo feels irritated.   \n",
       "3                           Alonzo feels enraged.   \n",
       "4                           Alonzo feels annoyed.   \n",
       "...                                           ...   \n",
       "8635      The conversation with my mom was funny.   \n",
       "8636  The conversation with my mom was hilarious.   \n",
       "8637    The conversation with my mom was amazing.   \n",
       "8638  The conversation with my mom was wonderful.   \n",
       "8639      The conversation with my mom was great.   \n",
       "\n",
       "                                  rawPrediction_x  prediction_x    id  \\\n",
       "0         [1.758761681156161, -1.758761681156161]           0.0     1   \n",
       "1       [1.1883767559140836, -1.1883767559140836]           0.0     2   \n",
       "2       [1.1883767559140836, -1.1883767559140836]           0.0     3   \n",
       "3       [1.1883767559140836, -1.1883767559140836]           0.0     4   \n",
       "4       [1.1883767559140836, -1.1883767559140836]           0.0     5   \n",
       "...                                           ...           ...   ...   \n",
       "8635  [-0.26419630122863746, 0.26419630122863746]           1.0  8636   \n",
       "8636  [0.47931264253825856, -0.47931264253825856]           0.0  8637   \n",
       "8637    [-1.2568624618888382, 1.2568624618888382]           1.0  8638   \n",
       "8638    [-1.1733344977347269, 1.1733344977347269]           1.0  8639   \n",
       "8639    [-1.5774363136664324, 1.5774363136664324]           1.0  8640   \n",
       "\n",
       "                                           text_y  \\\n",
       "0                             Alonzo feels angry.   \n",
       "1                           Alonzo feels furious.   \n",
       "2                         Alonzo feels irritated.   \n",
       "3                           Alonzo feels enraged.   \n",
       "4                           Alonzo feels annoyed.   \n",
       "...                                           ...   \n",
       "8635      The conversation with my mom was funny.   \n",
       "8636  The conversation with my mom was hilarious.   \n",
       "8637    The conversation with my mom was amazing.   \n",
       "8638  The conversation with my mom was wonderful.   \n",
       "8639      The conversation with my mom was great.   \n",
       "\n",
       "                              rawPrediction_y  prediction_y  new_val1_x  \\\n",
       "0     [8.581314520952962, 11.418685479047037]           1.0    1.758762   \n",
       "1     [8.581314520952962, 11.418685479047037]           1.0    1.188377   \n",
       "2     [8.581314520952962, 11.418685479047037]           1.0    1.188377   \n",
       "3     [8.581314520952962, 11.418685479047037]           1.0    1.188377   \n",
       "4     [8.581314520952962, 11.418685479047037]           1.0    1.188377   \n",
       "...                                       ...           ...         ...   \n",
       "8635  [8.581314520952962, 11.418685479047037]           1.0   -0.264196   \n",
       "8636  [8.581314520952962, 11.418685479047037]           1.0    0.479313   \n",
       "8637  [8.171052346622254, 11.828947653377746]           1.0   -1.256862   \n",
       "8638  [8.581314520952962, 11.418685479047037]           1.0   -1.173334   \n",
       "8639  [8.154866507877463, 11.845133492122535]           1.0   -1.577436   \n",
       "\n",
       "      new_val2_x  new_val1_y  new_val2_y  \n",
       "0       8.581315    0.524358    1.000000  \n",
       "1       8.581315    0.443084    1.000000  \n",
       "2       8.581315    0.443084    1.000000  \n",
       "3       8.581315    0.443084    1.000000  \n",
       "4       8.581315    0.443084    1.000000  \n",
       "...          ...         ...         ...  \n",
       "8635    8.581315    0.236106    1.000000  \n",
       "8636    8.581315    0.342049    1.000000  \n",
       "8637    8.171052    0.094661    0.037955  \n",
       "8638    8.581315    0.106563    1.000000  \n",
       "8639    8.154867    0.048983    0.000000  \n",
       "\n",
       "[8640 rows x 11 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "eec[\"id\"] = eec.index +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df.merge(eec, on= \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_x</th>\n",
       "      <th>rawPrediction_x</th>\n",
       "      <th>prediction_x</th>\n",
       "      <th>id</th>\n",
       "      <th>text_y</th>\n",
       "      <th>rawPrediction_y</th>\n",
       "      <th>prediction_y</th>\n",
       "      <th>new_val1_x</th>\n",
       "      <th>new_val2_x</th>\n",
       "      <th>new_val1_y</th>\n",
       "      <th>...</th>\n",
       "      <th>Emotion_x</th>\n",
       "      <th>Emotion word_x</th>\n",
       "      <th>ID_y</th>\n",
       "      <th>Sentence_y</th>\n",
       "      <th>Template_y</th>\n",
       "      <th>Person_y</th>\n",
       "      <th>Gender_y</th>\n",
       "      <th>Race_y</th>\n",
       "      <th>Emotion_y</th>\n",
       "      <th>Emotion word_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>[1.758761681156161, -1.758761681156161]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.758762</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.524358</td>\n",
       "      <td>...</td>\n",
       "      <td>anger</td>\n",
       "      <td>angry</td>\n",
       "      <td>2018-En-mystery-05498</td>\n",
       "      <td>Alonzo feels angry.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.443084</td>\n",
       "      <td>...</td>\n",
       "      <td>anger</td>\n",
       "      <td>furious</td>\n",
       "      <td>2018-En-mystery-11722</td>\n",
       "      <td>Alonzo feels furious.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>furious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.443084</td>\n",
       "      <td>...</td>\n",
       "      <td>anger</td>\n",
       "      <td>irritated</td>\n",
       "      <td>2018-En-mystery-11364</td>\n",
       "      <td>Alonzo feels irritated.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>irritated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.443084</td>\n",
       "      <td>...</td>\n",
       "      <td>anger</td>\n",
       "      <td>enraged</td>\n",
       "      <td>2018-En-mystery-14320</td>\n",
       "      <td>Alonzo feels enraged.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>enraged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>[1.1883767559140836, -1.1883767559140836]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.188377</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.443084</td>\n",
       "      <td>...</td>\n",
       "      <td>anger</td>\n",
       "      <td>annoyed</td>\n",
       "      <td>2018-En-mystery-14114</td>\n",
       "      <td>Alonzo feels annoyed.</td>\n",
       "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
       "      <td>Alonzo</td>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>annoyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>[-0.26419630122863746, 0.26419630122863746]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8636</td>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.264196</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.236106</td>\n",
       "      <td>...</td>\n",
       "      <td>joy</td>\n",
       "      <td>funny</td>\n",
       "      <td>2018-En-mystery-12020</td>\n",
       "      <td>The conversation with my mom was funny.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8636</th>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>[0.47931264253825856, -0.47931264253825856]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8637</td>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479313</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.342049</td>\n",
       "      <td>...</td>\n",
       "      <td>joy</td>\n",
       "      <td>hilarious</td>\n",
       "      <td>2018-En-mystery-14529</td>\n",
       "      <td>The conversation with my mom was hilarious.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>hilarious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8637</th>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>[-1.2568624618888382, 1.2568624618888382]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8638</td>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>[8.171052346622254, 11.828947653377746]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.256862</td>\n",
       "      <td>8.171052</td>\n",
       "      <td>0.094661</td>\n",
       "      <td>...</td>\n",
       "      <td>joy</td>\n",
       "      <td>amazing</td>\n",
       "      <td>2018-En-mystery-16746</td>\n",
       "      <td>The conversation with my mom was amazing.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638</th>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>[-1.1733344977347269, 1.1733344977347269]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8639</td>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>[8.581314520952962, 11.418685479047037]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.173334</td>\n",
       "      <td>8.581315</td>\n",
       "      <td>0.106563</td>\n",
       "      <td>...</td>\n",
       "      <td>joy</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>2018-En-mystery-00046</td>\n",
       "      <td>The conversation with my mom was wonderful.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>wonderful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>[-1.5774363136664324, 1.5774363136664324]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8640</td>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>[8.154866507877463, 11.845133492122535]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.577436</td>\n",
       "      <td>8.154867</td>\n",
       "      <td>0.048983</td>\n",
       "      <td>...</td>\n",
       "      <td>joy</td>\n",
       "      <td>great</td>\n",
       "      <td>2018-En-mystery-16664</td>\n",
       "      <td>The conversation with my mom was great.</td>\n",
       "      <td>The conversation with &lt;person object&gt; was &lt;emo...</td>\n",
       "      <td>my mom</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joy</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8640 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_x  \\\n",
       "0                             Alonzo feels angry.   \n",
       "1                           Alonzo feels furious.   \n",
       "2                         Alonzo feels irritated.   \n",
       "3                           Alonzo feels enraged.   \n",
       "4                           Alonzo feels annoyed.   \n",
       "...                                           ...   \n",
       "8635      The conversation with my mom was funny.   \n",
       "8636  The conversation with my mom was hilarious.   \n",
       "8637    The conversation with my mom was amazing.   \n",
       "8638  The conversation with my mom was wonderful.   \n",
       "8639      The conversation with my mom was great.   \n",
       "\n",
       "                                  rawPrediction_x  prediction_x    id  \\\n",
       "0         [1.758761681156161, -1.758761681156161]           0.0     1   \n",
       "1       [1.1883767559140836, -1.1883767559140836]           0.0     2   \n",
       "2       [1.1883767559140836, -1.1883767559140836]           0.0     3   \n",
       "3       [1.1883767559140836, -1.1883767559140836]           0.0     4   \n",
       "4       [1.1883767559140836, -1.1883767559140836]           0.0     5   \n",
       "...                                           ...           ...   ...   \n",
       "8635  [-0.26419630122863746, 0.26419630122863746]           1.0  8636   \n",
       "8636  [0.47931264253825856, -0.47931264253825856]           0.0  8637   \n",
       "8637    [-1.2568624618888382, 1.2568624618888382]           1.0  8638   \n",
       "8638    [-1.1733344977347269, 1.1733344977347269]           1.0  8639   \n",
       "8639    [-1.5774363136664324, 1.5774363136664324]           1.0  8640   \n",
       "\n",
       "                                           text_y  \\\n",
       "0                             Alonzo feels angry.   \n",
       "1                           Alonzo feels furious.   \n",
       "2                         Alonzo feels irritated.   \n",
       "3                           Alonzo feels enraged.   \n",
       "4                           Alonzo feels annoyed.   \n",
       "...                                           ...   \n",
       "8635      The conversation with my mom was funny.   \n",
       "8636  The conversation with my mom was hilarious.   \n",
       "8637    The conversation with my mom was amazing.   \n",
       "8638  The conversation with my mom was wonderful.   \n",
       "8639      The conversation with my mom was great.   \n",
       "\n",
       "                              rawPrediction_y  prediction_y  new_val1_x  \\\n",
       "0     [8.581314520952962, 11.418685479047037]           1.0    1.758762   \n",
       "1     [8.581314520952962, 11.418685479047037]           1.0    1.188377   \n",
       "2     [8.581314520952962, 11.418685479047037]           1.0    1.188377   \n",
       "3     [8.581314520952962, 11.418685479047037]           1.0    1.188377   \n",
       "4     [8.581314520952962, 11.418685479047037]           1.0    1.188377   \n",
       "...                                       ...           ...         ...   \n",
       "8635  [8.581314520952962, 11.418685479047037]           1.0   -0.264196   \n",
       "8636  [8.581314520952962, 11.418685479047037]           1.0    0.479313   \n",
       "8637  [8.171052346622254, 11.828947653377746]           1.0   -1.256862   \n",
       "8638  [8.581314520952962, 11.418685479047037]           1.0   -1.173334   \n",
       "8639  [8.154866507877463, 11.845133492122535]           1.0   -1.577436   \n",
       "\n",
       "      new_val2_x  new_val1_y  ...  Emotion_x Emotion word_x  \\\n",
       "0       8.581315    0.524358  ...      anger          angry   \n",
       "1       8.581315    0.443084  ...      anger        furious   \n",
       "2       8.581315    0.443084  ...      anger      irritated   \n",
       "3       8.581315    0.443084  ...      anger        enraged   \n",
       "4       8.581315    0.443084  ...      anger        annoyed   \n",
       "...          ...         ...  ...        ...            ...   \n",
       "8635    8.581315    0.236106  ...        joy          funny   \n",
       "8636    8.581315    0.342049  ...        joy      hilarious   \n",
       "8637    8.171052    0.094661  ...        joy        amazing   \n",
       "8638    8.581315    0.106563  ...        joy      wonderful   \n",
       "8639    8.154867    0.048983  ...        joy          great   \n",
       "\n",
       "                       ID_y                                   Sentence_y  \\\n",
       "0     2018-En-mystery-05498                          Alonzo feels angry.   \n",
       "1     2018-En-mystery-11722                        Alonzo feels furious.   \n",
       "2     2018-En-mystery-11364                      Alonzo feels irritated.   \n",
       "3     2018-En-mystery-14320                        Alonzo feels enraged.   \n",
       "4     2018-En-mystery-14114                        Alonzo feels annoyed.   \n",
       "...                     ...                                          ...   \n",
       "8635  2018-En-mystery-12020      The conversation with my mom was funny.   \n",
       "8636  2018-En-mystery-14529  The conversation with my mom was hilarious.   \n",
       "8637  2018-En-mystery-16746    The conversation with my mom was amazing.   \n",
       "8638  2018-En-mystery-00046  The conversation with my mom was wonderful.   \n",
       "8639  2018-En-mystery-16664      The conversation with my mom was great.   \n",
       "\n",
       "                                             Template_y Person_y Gender_y  \\\n",
       "0                <person subject> feels <emotion word>.   Alonzo     male   \n",
       "1                <person subject> feels <emotion word>.   Alonzo     male   \n",
       "2                <person subject> feels <emotion word>.   Alonzo     male   \n",
       "3                <person subject> feels <emotion word>.   Alonzo     male   \n",
       "4                <person subject> feels <emotion word>.   Alonzo     male   \n",
       "...                                                 ...      ...      ...   \n",
       "8635  The conversation with <person object> was <emo...   my mom   female   \n",
       "8636  The conversation with <person object> was <emo...   my mom   female   \n",
       "8637  The conversation with <person object> was <emo...   my mom   female   \n",
       "8638  The conversation with <person object> was <emo...   my mom   female   \n",
       "8639  The conversation with <person object> was <emo...   my mom   female   \n",
       "\n",
       "                Race_y Emotion_y Emotion word_y  \n",
       "0     African-American     anger          angry  \n",
       "1     African-American     anger        furious  \n",
       "2     African-American     anger      irritated  \n",
       "3     African-American     anger        enraged  \n",
       "4     African-American     anger        annoyed  \n",
       "...                ...       ...            ...  \n",
       "8635               NaN       joy          funny  \n",
       "8636               NaN       joy      hilarious  \n",
       "8637               NaN       joy        amazing  \n",
       "8638               NaN       joy      wonderful  \n",
       "8639               NaN       joy          great  \n",
       "\n",
       "[8640 rows x 27 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                 0\n",
       "Sentence           0\n",
       "Template           0\n",
       "Person             0\n",
       "Gender             0\n",
       "Race            2880\n",
       "Emotion          240\n",
       "Emotion word     240\n",
       "id                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eec.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.to_csv(\"final_eec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_val1_y</th>\n",
       "      <th>new_val2_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>0.449736</td>\n",
       "      <td>0.777712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.447877</td>\n",
       "      <td>0.777712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        new_val1_y  new_val2_y\n",
       "Gender                        \n",
       "female    0.449736    0.777712\n",
       "male      0.447877    0.777712"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby('Gender').agg({'new_val1_y': 'mean' , 'new_val2_y': 'mean'})\n",
    "#the results show bias in gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_val1_y</th>\n",
       "      <th>new_val2_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Race</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>African-American</th>\n",
       "      <td>0.450365</td>\n",
       "      <td>0.777712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>European</th>\n",
       "      <td>0.448027</td>\n",
       "      <td>0.777712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  new_val1_y  new_val2_y\n",
       "Race                                    \n",
       "African-American    0.450365    0.777712\n",
       "European            0.448027    0.777712"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.groupby('Race').agg({'new_val1_y': 'mean' , 'new_val2_y': 'mean'})\n",
    "#the results show bias in race\n",
    "#the new_val1_y is avg scaled sentiment score from LR\n",
    "#the new_val2_y is avg scaled sentiment score from random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>ag_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0.453154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>European</td>\n",
       "      <td>0.448027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0.447577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>European</td>\n",
       "      <td>0.448027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender              Race   ag_mean\n",
       "0  female  African-American  0.453154\n",
       "1  female          European  0.448027\n",
       "2    male  African-American  0.447577\n",
       "3    male          European  0.448027"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_multiple = final_df.groupby(['Gender', 'Race']).agg({'new_val1_y': ['mean']})\n",
    "grouped_multiple.columns = ['ag_mean']\n",
    "grouped_multiple = grouped_multiple.reset_index()\n",
    "grouped_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>ag_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.482484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.575225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.199898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.556666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>European</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.477358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female</td>\n",
       "      <td>European</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.570099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female</td>\n",
       "      <td>European</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.194771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female</td>\n",
       "      <td>European</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.551540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.476907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.569648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.194321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.551089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>male</td>\n",
       "      <td>European</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.477358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>male</td>\n",
       "      <td>European</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.570099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>male</td>\n",
       "      <td>European</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.194771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>male</td>\n",
       "      <td>European</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.551540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gender              Race  Emotion   ag_mean\n",
       "0   female  African-American    anger  0.482484\n",
       "1   female  African-American     fear  0.575225\n",
       "2   female  African-American      joy  0.199898\n",
       "3   female  African-American  sadness  0.556666\n",
       "4   female          European    anger  0.477358\n",
       "5   female          European     fear  0.570099\n",
       "6   female          European      joy  0.194771\n",
       "7   female          European  sadness  0.551540\n",
       "8     male  African-American    anger  0.476907\n",
       "9     male  African-American     fear  0.569648\n",
       "10    male  African-American      joy  0.194321\n",
       "11    male  African-American  sadness  0.551089\n",
       "12    male          European    anger  0.477358\n",
       "13    male          European     fear  0.570099\n",
       "14    male          European      joy  0.194771\n",
       "15    male          European  sadness  0.551540"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_multiple = final_df.groupby(['Gender', 'Race', 'Emotion']).agg({'new_val1_y': ['mean']})\n",
    "grouped_multiple.columns = ['ag_mean']\n",
    "grouped_multiple = grouped_multiple.reset_index()\n",
    "grouped_multiple\n",
    "#the below results show the gap between all emotions, race, and gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
